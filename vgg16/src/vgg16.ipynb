{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3548c0-3274-4dcb-ba4a-65e1de19f317",
   "metadata": {
    "id": "9d3548c0-3274-4dcb-ba4a-65e1de19f317",
    "tags": []
   },
   "source": [
    "# پروژه پایانی"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d0e3c3-dbd3-4ee1-8fe6-604e22c8bb21",
   "metadata": {
    "id": "f6d0e3c3-dbd3-4ee1-8fe6-604e22c8bb21"
   },
   "source": [
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200219152327/conv-layers-vgg16.jpg\" alt=\"VGG 16 Architecture\" width=500px>\n",
    "<img src=\"./pic/vgg16.png\" alt=\"VGG 16 Architecture\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2eefe9-c5bb-45fe-a21f-d159f527dc16",
   "metadata": {
    "id": "3b2eefe9-c5bb-45fe-a21f-d159f527dc16",
    "tags": []
   },
   "source": [
    "## افزودن کتابخانه‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c53e635-b968-482b-a95f-c3edb635f4e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-01-29T08:54:26.912763Z",
     "iopub.status.busy": "2024-01-29T08:54:26.912412Z",
     "iopub.status.idle": "2024-01-29T08:54:51.123308Z",
     "shell.execute_reply": "2024-01-29T08:54:51.122601Z",
     "shell.execute_reply.started": "2024-01-29T08:54:26.912734Z"
    },
    "id": "1c53e635-b968-482b-a95f-c3edb635f4e3",
    "outputId": "91f91944-95e9-436f-b460-c3a0286d3adf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn # برای پیاده‌سازی معماری شبکه\n",
    "from torchvision import datasets # موجود بودن دیتاست مورد نظر در این کتابخانه\n",
    "from torchvision import transforms # برای کارهای پیش‌پردازش تصویر\n",
    "from torch.utils.data.sampler import SubsetRandomSampler # نمونه‌برداری تصادفی\n",
    "\n",
    "\n",
    "# انتخاب نوع پردازنده\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1ab697-94d3-4557-814f-74b9ea754910",
   "metadata": {
    "id": "fd1ab697-94d3-4557-814f-74b9ea754910"
   },
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5385afdf-8693-4278-8504-fd8befa8206e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T08:57:44.774229Z",
     "iopub.status.busy": "2024-01-29T08:57:44.773529Z",
     "iopub.status.idle": "2024-01-29T08:57:44.781183Z",
     "shell.execute_reply": "2024-01-29T08:57:44.780625Z",
     "shell.execute_reply.started": "2024-01-29T08:57:44.774204Z"
    },
    "id": "5385afdf-8693-4278-8504-fd8befa8206e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_loader(data_dir, batch_size, random_seed=123, valid_size=0.1, shuffle=True, test=False):\n",
    "\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # پیش‌پردازش تصویر\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(), # تبدیل به تسنور برای استفاده از جی‌پی‌یو\n",
    "            normalize,\n",
    "    ])\n",
    "\n",
    "    if test:\n",
    "        dataset = datasets.CIFAR100(\n",
    "          root=data_dir, train=False,\n",
    "          download=True, transform=transform,\n",
    "        )\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = datasets.CIFAR100(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22308ed0-6bfc-480e-98c6-386c97ce2b2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-01-29T08:58:19.313109Z",
     "iopub.status.busy": "2024-01-29T08:58:19.312395Z",
     "iopub.status.idle": "2024-01-29T09:06:10.202444Z",
     "shell.execute_reply": "2024-01-29T09:06:10.201443Z",
     "shell.execute_reply.started": "2024-01-29T08:58:19.313044Z"
    },
    "id": "22308ed0-6bfc-480e-98c6-386c97ce2b2e",
    "outputId": "22fb598f-ebe1-4935-8859-f6d70bca259c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR100 دیتاست\n",
    "train_loader, valid_loader = data_loader(data_dir='./data', batch_size=64)\n",
    "\n",
    "test_loader = data_loader(data_dir='./data', batch_size=64, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daebd10e-dba7-4461-b7e4-b603386572e1",
   "metadata": {
    "id": "daebd10e-dba7-4461-b7e4-b603386572e1"
   },
   "source": [
    "## VGG Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47a7740-446a-457b-9ab8-533296eccf7c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-29T09:06:10.203175Z",
     "iopub.status.idle": "2024-01-29T09:06:10.203423Z",
     "shell.execute_reply": "2024-01-29T09:06:10.203314Z",
     "shell.execute_reply.started": "2024-01-29T09:06:10.203301Z"
    },
    "id": "b47a7740-446a-457b-9ab8-533296eccf7c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(7*7*512, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = self.layer9(out)\n",
    "        out = self.layer10(out)\n",
    "        out = self.layer11(out)\n",
    "        out = self.layer12(out)\n",
    "        out = self.layer13(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cfb6e4-4555-4aa9-a86c-04138e125953",
   "metadata": {
    "id": "53cfb6e4-4555-4aa9-a86c-04138e125953"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f40222bc-6759-4d8b-ba68-8651afe9991b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-29T09:06:10.204262Z",
     "iopub.status.idle": "2024-01-29T09:06:10.204517Z",
     "shell.execute_reply": "2024-01-29T09:06:10.204405Z",
     "shell.execute_reply.started": "2024-01-29T09:06:10.204393Z"
    },
    "id": "f40222bc-6759-4d8b-ba68-8651afe9991b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 100\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = VGG16(num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.005, momentum=0.9)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9fbbb1-c027-4126-b4e8-15866817ecc3",
   "metadata": {
    "id": "ee9fbbb1-c027-4126-b4e8-15866817ecc3"
   },
   "source": [
    "## آموزش - Train\n",
    "> به دلیل حجیم بودن این معماری، نزدیک به چهارده میلیون پارامتر، این که مدل را از صفر آموزش دهیم باعث صرف زمان خیلی خیلی زیادی میشود. به همین دلیل تا اندکی از را اجرا کرده‌ام"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d8c5951-feb2-4a68-9b03-369d91bd7466",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "5d8c5951-feb2-4a68-9b03-369d91bd7466",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a7287525-5ab5-4d6b-d95f-1187a97af717",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1/704], Loss: 4.6159\n",
      "Epoch [1/20], Step [2/704], Loss: 4.6327\n",
      "Epoch [1/20], Step [3/704], Loss: 4.6542\n",
      "Epoch [1/20], Step [4/704], Loss: 4.6874\n",
      "Epoch [1/20], Step [5/704], Loss: 4.6634\n",
      "Epoch [1/20], Step [6/704], Loss: 4.6515\n",
      "Epoch [1/20], Step [7/704], Loss: 4.6271\n",
      "Epoch [1/20], Step [8/704], Loss: 4.6674\n",
      "Epoch [1/20], Step [9/704], Loss: 4.6177\n",
      "Epoch [1/20], Step [10/704], Loss: 4.6153\n",
      "Epoch [1/20], Step [11/704], Loss: 4.6632\n",
      "Epoch [1/20], Step [12/704], Loss: 4.6545\n",
      "Epoch [1/20], Step [13/704], Loss: 4.6506\n",
      "Epoch [1/20], Step [14/704], Loss: 4.6200\n",
      "Epoch [1/20], Step [15/704], Loss: 4.6399\n",
      "Epoch [1/20], Step [16/704], Loss: 4.5816\n",
      "Epoch [1/20], Step [17/704], Loss: 4.5591\n",
      "Epoch [1/20], Step [18/704], Loss: 4.5742\n",
      "Epoch [1/20], Step [19/704], Loss: 4.6781\n",
      "Epoch [1/20], Step [20/704], Loss: 4.6258\n",
      "Epoch [1/20], Step [21/704], Loss: 4.5796\n",
      "Epoch [1/20], Step [22/704], Loss: 4.5994\n",
      "Epoch [1/20], Step [23/704], Loss: 4.7216\n",
      "Epoch [1/20], Step [24/704], Loss: 4.6748\n",
      "Epoch [1/20], Step [25/704], Loss: 4.5209\n",
      "Epoch [1/20], Step [26/704], Loss: 4.5637\n",
      "Epoch [1/20], Step [27/704], Loss: 4.7362\n",
      "Epoch [1/20], Step [28/704], Loss: 4.6050\n",
      "Epoch [1/20], Step [29/704], Loss: 4.4990\n",
      "Epoch [1/20], Step [30/704], Loss: 4.6239\n",
      "Epoch [1/20], Step [31/704], Loss: 4.6191\n",
      "Epoch [1/20], Step [32/704], Loss: 4.5383\n",
      "Epoch [1/20], Step [33/704], Loss: 4.6316\n",
      "Epoch [1/20], Step [34/704], Loss: 4.4743\n",
      "Epoch [1/20], Step [35/704], Loss: 4.4912\n",
      "Epoch [1/20], Step [36/704], Loss: 4.5065\n",
      "Epoch [1/20], Step [37/704], Loss: 4.6004\n",
      "Epoch [1/20], Step [38/704], Loss: 4.6329\n",
      "Epoch [1/20], Step [39/704], Loss: 4.4413\n",
      "Epoch [1/20], Step [40/704], Loss: 4.6129\n",
      "Epoch [1/20], Step [41/704], Loss: 4.6675\n",
      "Epoch [1/20], Step [42/704], Loss: 4.5434\n",
      "Epoch [1/20], Step [43/704], Loss: 4.5112\n",
      "Epoch [1/20], Step [44/704], Loss: 4.4334\n",
      "Epoch [1/20], Step [45/704], Loss: 4.5346\n",
      "Epoch [1/20], Step [46/704], Loss: 4.5080\n",
      "Epoch [1/20], Step [47/704], Loss: 4.5608\n",
      "Epoch [1/20], Step [48/704], Loss: 4.5121\n",
      "Epoch [1/20], Step [49/704], Loss: 4.4818\n",
      "Epoch [1/20], Step [50/704], Loss: 4.3848\n",
      "Epoch [1/20], Step [51/704], Loss: 4.7066\n",
      "Epoch [1/20], Step [52/704], Loss: 4.6048\n",
      "Epoch [1/20], Step [53/704], Loss: 4.5752\n",
      "Epoch [1/20], Step [54/704], Loss: 4.6469\n",
      "Epoch [1/20], Step [55/704], Loss: 4.7023\n",
      "Epoch [1/20], Step [56/704], Loss: 4.4691\n",
      "Epoch [1/20], Step [57/704], Loss: 4.5268\n",
      "Epoch [1/20], Step [58/704], Loss: 4.5532\n",
      "Epoch [1/20], Step [59/704], Loss: 4.5824\n",
      "Epoch [1/20], Step [60/704], Loss: 4.5517\n",
      "Epoch [1/20], Step [61/704], Loss: 4.4889\n",
      "Epoch [1/20], Step [62/704], Loss: 4.4573\n",
      "Epoch [1/20], Step [63/704], Loss: 4.4375\n",
      "Epoch [1/20], Step [64/704], Loss: 4.4940\n",
      "Epoch [1/20], Step [65/704], Loss: 4.5934\n",
      "Epoch [1/20], Step [66/704], Loss: 4.4765\n",
      "Epoch [1/20], Step [67/704], Loss: 4.5114\n",
      "Epoch [1/20], Step [68/704], Loss: 4.4768\n",
      "Epoch [1/20], Step [69/704], Loss: 4.4379\n",
      "Epoch [1/20], Step [70/704], Loss: 4.5184\n",
      "Epoch [1/20], Step [71/704], Loss: 4.4899\n",
      "Epoch [1/20], Step [72/704], Loss: 4.4534\n",
      "Epoch [1/20], Step [73/704], Loss: 4.5397\n",
      "Epoch [1/20], Step [74/704], Loss: 4.4477\n",
      "Epoch [1/20], Step [75/704], Loss: 4.4628\n",
      "Epoch [1/20], Step [76/704], Loss: 4.5773\n",
      "Epoch [1/20], Step [77/704], Loss: 4.4701\n",
      "Epoch [1/20], Step [78/704], Loss: 4.5793\n",
      "Epoch [1/20], Step [79/704], Loss: 4.5121\n",
      "Epoch [1/20], Step [80/704], Loss: 4.5806\n",
      "Epoch [1/20], Step [81/704], Loss: 4.4896\n",
      "Epoch [1/20], Step [82/704], Loss: 4.4393\n",
      "Epoch [1/20], Step [83/704], Loss: 4.3732\n",
      "Epoch [1/20], Step [84/704], Loss: 4.5063\n",
      "Epoch [1/20], Step [85/704], Loss: 4.4556\n",
      "Epoch [1/20], Step [86/704], Loss: 4.4640\n",
      "Epoch [1/20], Step [87/704], Loss: 4.4801\n",
      "Epoch [1/20], Step [88/704], Loss: 4.4974\n",
      "Epoch [1/20], Step [89/704], Loss: 4.5303\n",
      "Epoch [1/20], Step [90/704], Loss: 4.4762\n",
      "Epoch [1/20], Step [91/704], Loss: 4.5084\n",
      "Epoch [1/20], Step [92/704], Loss: 4.4853\n",
      "Epoch [1/20], Step [93/704], Loss: 4.4097\n",
      "Epoch [1/20], Step [94/704], Loss: 4.3232\n",
      "Epoch [1/20], Step [95/704], Loss: 4.5172\n",
      "Epoch [1/20], Step [96/704], Loss: 4.4219\n",
      "Epoch [1/20], Step [97/704], Loss: 4.5613\n",
      "Epoch [1/20], Step [98/704], Loss: 4.4705\n",
      "Epoch [1/20], Step [99/704], Loss: 4.4269\n",
      "Epoch [1/20], Step [100/704], Loss: 4.4212\n",
      "Epoch [1/20], Step [101/704], Loss: 4.5327\n",
      "Epoch [1/20], Step [102/704], Loss: 4.3964\n",
      "Epoch [1/20], Step [103/704], Loss: 4.4370\n",
      "Epoch [1/20], Step [104/704], Loss: 4.4416\n",
      "Epoch [1/20], Step [105/704], Loss: 4.4653\n",
      "Epoch [1/20], Step [106/704], Loss: 4.4364\n",
      "Epoch [1/20], Step [107/704], Loss: 4.4153\n",
      "Epoch [1/20], Step [108/704], Loss: 4.3515\n",
      "Epoch [1/20], Step [109/704], Loss: 4.4630\n",
      "Epoch [1/20], Step [110/704], Loss: 4.4981\n",
      "Epoch [1/20], Step [111/704], Loss: 4.4548\n",
      "Epoch [1/20], Step [112/704], Loss: 4.4855\n",
      "Epoch [1/20], Step [113/704], Loss: 4.4870\n",
      "Epoch [1/20], Step [114/704], Loss: 4.3420\n",
      "Epoch [1/20], Step [115/704], Loss: 4.3807\n",
      "Epoch [1/20], Step [116/704], Loss: 4.3838\n",
      "Epoch [1/20], Step [117/704], Loss: 4.5137\n",
      "Epoch [1/20], Step [118/704], Loss: 4.2714\n",
      "Epoch [1/20], Step [119/704], Loss: 4.4201\n",
      "Epoch [1/20], Step [120/704], Loss: 4.3571\n",
      "Epoch [1/20], Step [121/704], Loss: 4.3900\n",
      "Epoch [1/20], Step [122/704], Loss: 4.3743\n",
      "Epoch [1/20], Step [123/704], Loss: 4.4500\n",
      "Epoch [1/20], Step [124/704], Loss: 4.4002\n",
      "Epoch [1/20], Step [125/704], Loss: 4.3295\n",
      "Epoch [1/20], Step [126/704], Loss: 4.6579\n",
      "Epoch [1/20], Step [127/704], Loss: 4.2759\n",
      "Epoch [1/20], Step [128/704], Loss: 4.2950\n",
      "Epoch [1/20], Step [129/704], Loss: 4.2924\n",
      "Epoch [1/20], Step [130/704], Loss: 4.3804\n",
      "Epoch [1/20], Step [131/704], Loss: 4.4344\n",
      "Epoch [1/20], Step [132/704], Loss: 4.3697\n",
      "Epoch [1/20], Step [133/704], Loss: 4.3393\n",
      "Epoch [1/20], Step [134/704], Loss: 4.3509\n",
      "Epoch [1/20], Step [135/704], Loss: 4.4389\n",
      "Epoch [1/20], Step [136/704], Loss: 4.3296\n",
      "Epoch [1/20], Step [137/704], Loss: 4.4315\n",
      "Epoch [1/20], Step [138/704], Loss: 4.2632\n",
      "Epoch [1/20], Step [139/704], Loss: 4.2739\n",
      "Epoch [1/20], Step [140/704], Loss: 4.2612\n",
      "Epoch [1/20], Step [141/704], Loss: 4.1984\n",
      "Epoch [1/20], Step [142/704], Loss: 4.3864\n",
      "Epoch [1/20], Step [143/704], Loss: 4.5366\n",
      "Epoch [1/20], Step [144/704], Loss: 4.3513\n",
      "Epoch [1/20], Step [145/704], Loss: 4.3889\n",
      "Epoch [1/20], Step [146/704], Loss: 4.2949\n",
      "Epoch [1/20], Step [147/704], Loss: 4.1338\n",
      "Epoch [1/20], Step [148/704], Loss: 4.3204\n",
      "Epoch [1/20], Step [149/704], Loss: 4.2078\n",
      "Epoch [1/20], Step [150/704], Loss: 4.2414\n",
      "Epoch [1/20], Step [151/704], Loss: 4.2631\n",
      "Epoch [1/20], Step [152/704], Loss: 4.3372\n",
      "Epoch [1/20], Step [153/704], Loss: 4.3015\n",
      "Epoch [1/20], Step [154/704], Loss: 4.3804\n",
      "Epoch [1/20], Step [155/704], Loss: 4.1587\n",
      "Epoch [1/20], Step [156/704], Loss: 4.1195\n",
      "Epoch [1/20], Step [157/704], Loss: 4.3956\n",
      "Epoch [1/20], Step [158/704], Loss: 4.3776\n",
      "Epoch [1/20], Step [159/704], Loss: 4.2860\n",
      "Epoch [1/20], Step [160/704], Loss: 4.2339\n",
      "Epoch [1/20], Step [161/704], Loss: 4.3223\n",
      "Epoch [1/20], Step [162/704], Loss: 4.2215\n",
      "Epoch [1/20], Step [163/704], Loss: 4.2811\n",
      "Epoch [1/20], Step [164/704], Loss: 4.0534\n",
      "Epoch [1/20], Step [165/704], Loss: 4.0413\n",
      "Epoch [1/20], Step [166/704], Loss: 4.1357\n",
      "Epoch [1/20], Step [167/704], Loss: 4.3660\n",
      "Epoch [1/20], Step [168/704], Loss: 4.2681\n",
      "Epoch [1/20], Step [169/704], Loss: 4.1054\n",
      "Epoch [1/20], Step [170/704], Loss: 4.3964\n",
      "Epoch [1/20], Step [171/704], Loss: 4.3262\n",
      "Epoch [1/20], Step [172/704], Loss: 4.1559\n",
      "Epoch [1/20], Step [173/704], Loss: 4.2308\n",
      "Epoch [1/20], Step [174/704], Loss: 4.2077\n",
      "Epoch [1/20], Step [175/704], Loss: 4.3232\n",
      "Epoch [1/20], Step [176/704], Loss: 4.1741\n",
      "Epoch [1/20], Step [177/704], Loss: 4.4252\n",
      "Epoch [1/20], Step [178/704], Loss: 4.1677\n",
      "Epoch [1/20], Step [179/704], Loss: 4.0758\n",
      "Epoch [1/20], Step [180/704], Loss: 4.3425\n",
      "Epoch [1/20], Step [181/704], Loss: 4.3272\n",
      "Epoch [1/20], Step [182/704], Loss: 4.2438\n",
      "Epoch [1/20], Step [183/704], Loss: 4.2073\n",
      "Epoch [1/20], Step [184/704], Loss: 4.2088\n",
      "Epoch [1/20], Step [185/704], Loss: 4.1410\n",
      "Epoch [1/20], Step [186/704], Loss: 4.2960\n",
      "Epoch [1/20], Step [187/704], Loss: 4.2163\n",
      "Epoch [1/20], Step [188/704], Loss: 3.9972\n",
      "Epoch [1/20], Step [189/704], Loss: 4.0815\n",
      "Epoch [1/20], Step [190/704], Loss: 4.2231\n",
      "Epoch [1/20], Step [191/704], Loss: 4.0094\n",
      "Epoch [1/20], Step [192/704], Loss: 4.0827\n",
      "Epoch [1/20], Step [193/704], Loss: 4.1818\n",
      "Epoch [1/20], Step [194/704], Loss: 4.1983\n",
      "Epoch [1/20], Step [195/704], Loss: 4.1759\n",
      "Epoch [1/20], Step [196/704], Loss: 4.2996\n",
      "Epoch [1/20], Step [197/704], Loss: 4.3124\n",
      "Epoch [1/20], Step [198/704], Loss: 4.3058\n",
      "Epoch [1/20], Step [199/704], Loss: 4.0328\n",
      "Epoch [1/20], Step [200/704], Loss: 4.0918\n",
      "Epoch [1/20], Step [201/704], Loss: 4.0674\n",
      "Epoch [1/20], Step [202/704], Loss: 4.1386\n",
      "Epoch [1/20], Step [203/704], Loss: 4.0797\n",
      "Epoch [1/20], Step [204/704], Loss: 4.2428\n",
      "Epoch [1/20], Step [205/704], Loss: 4.1174\n",
      "Epoch [1/20], Step [206/704], Loss: 4.3446\n",
      "Epoch [1/20], Step [207/704], Loss: 4.1813\n",
      "Epoch [1/20], Step [208/704], Loss: 4.0276\n",
      "Epoch [1/20], Step [209/704], Loss: 4.1373\n",
      "Epoch [1/20], Step [210/704], Loss: 4.0687\n",
      "Epoch [1/20], Step [211/704], Loss: 4.2891\n",
      "Epoch [1/20], Step [212/704], Loss: 4.2328\n",
      "Epoch [1/20], Step [213/704], Loss: 4.0420\n",
      "Epoch [1/20], Step [214/704], Loss: 4.1320\n",
      "Epoch [1/20], Step [215/704], Loss: 4.0073\n",
      "Epoch [1/20], Step [216/704], Loss: 4.1039\n",
      "Epoch [1/20], Step [217/704], Loss: 4.0397\n",
      "Epoch [1/20], Step [218/704], Loss: 4.1935\n",
      "Epoch [1/20], Step [219/704], Loss: 4.2458\n",
      "Epoch [1/20], Step [220/704], Loss: 3.8161\n",
      "Epoch [1/20], Step [221/704], Loss: 4.1013\n",
      "Epoch [1/20], Step [222/704], Loss: 4.1094\n",
      "Epoch [1/20], Step [223/704], Loss: 4.3167\n",
      "Epoch [1/20], Step [224/704], Loss: 4.2152\n",
      "Epoch [1/20], Step [225/704], Loss: 4.2892\n",
      "Epoch [1/20], Step [226/704], Loss: 4.2787\n",
      "Epoch [1/20], Step [227/704], Loss: 4.1346\n",
      "Epoch [1/20], Step [228/704], Loss: 4.3069\n",
      "Epoch [1/20], Step [229/704], Loss: 4.0102\n",
      "Epoch [1/20], Step [230/704], Loss: 4.1001\n",
      "Epoch [1/20], Step [231/704], Loss: 4.2699\n",
      "Epoch [1/20], Step [232/704], Loss: 3.9548\n",
      "Epoch [1/20], Step [233/704], Loss: 4.0370\n",
      "Epoch [1/20], Step [234/704], Loss: 4.1224\n",
      "Epoch [1/20], Step [235/704], Loss: 4.0946\n",
      "Epoch [1/20], Step [236/704], Loss: 4.0964\n",
      "Epoch [1/20], Step [237/704], Loss: 4.2707\n",
      "Epoch [1/20], Step [238/704], Loss: 4.1537\n",
      "Epoch [1/20], Step [239/704], Loss: 3.9918\n",
      "Epoch [1/20], Step [240/704], Loss: 4.2245\n",
      "Epoch [1/20], Step [241/704], Loss: 4.0931\n",
      "Epoch [1/20], Step [242/704], Loss: 4.1748\n",
      "Epoch [1/20], Step [243/704], Loss: 3.9729\n",
      "Epoch [1/20], Step [244/704], Loss: 4.0350\n",
      "Epoch [1/20], Step [245/704], Loss: 3.9914\n",
      "Epoch [1/20], Step [246/704], Loss: 4.2446\n",
      "Epoch [1/20], Step [247/704], Loss: 4.1077\n",
      "Epoch [1/20], Step [248/704], Loss: 4.2290\n",
      "Epoch [1/20], Step [249/704], Loss: 4.2855\n",
      "Epoch [1/20], Step [250/704], Loss: 4.1301\n",
      "Epoch [1/20], Step [251/704], Loss: 3.9735\n",
      "Epoch [1/20], Step [252/704], Loss: 4.1322\n",
      "Epoch [1/20], Step [253/704], Loss: 4.1128\n",
      "Epoch [1/20], Step [254/704], Loss: 4.0384\n",
      "Epoch [1/20], Step [255/704], Loss: 4.2352\n",
      "Epoch [1/20], Step [256/704], Loss: 3.9484\n",
      "Epoch [1/20], Step [257/704], Loss: 4.1662\n",
      "Epoch [1/20], Step [258/704], Loss: 3.8408\n",
      "Epoch [1/20], Step [259/704], Loss: 3.9466\n",
      "Epoch [1/20], Step [260/704], Loss: 3.8188\n",
      "Epoch [1/20], Step [261/704], Loss: 4.1801\n",
      "Epoch [1/20], Step [262/704], Loss: 3.9093\n",
      "Epoch [1/20], Step [263/704], Loss: 3.9242\n",
      "Epoch [1/20], Step [264/704], Loss: 3.8167\n",
      "Epoch [1/20], Step [265/704], Loss: 3.7993\n",
      "Epoch [1/20], Step [266/704], Loss: 4.0992\n",
      "Epoch [1/20], Step [267/704], Loss: 3.9286\n",
      "Epoch [1/20], Step [268/704], Loss: 3.8865\n",
      "Epoch [1/20], Step [269/704], Loss: 3.9866\n",
      "Epoch [1/20], Step [270/704], Loss: 3.7870\n",
      "Epoch [1/20], Step [271/704], Loss: 3.7565\n",
      "Epoch [1/20], Step [272/704], Loss: 3.8872\n",
      "Epoch [1/20], Step [273/704], Loss: 3.9707\n",
      "Epoch [1/20], Step [274/704], Loss: 4.3052\n",
      "Epoch [1/20], Step [275/704], Loss: 3.6986\n",
      "Epoch [1/20], Step [276/704], Loss: 4.1933\n",
      "Epoch [1/20], Step [277/704], Loss: 4.1629\n",
      "Epoch [1/20], Step [278/704], Loss: 4.0053\n",
      "Epoch [1/20], Step [279/704], Loss: 3.7638\n",
      "Epoch [1/20], Step [280/704], Loss: 3.7410\n",
      "Epoch [1/20], Step [281/704], Loss: 4.0847\n",
      "Epoch [1/20], Step [282/704], Loss: 4.1130\n",
      "Epoch [1/20], Step [283/704], Loss: 3.7665\n",
      "Epoch [1/20], Step [284/704], Loss: 4.2021\n",
      "Epoch [1/20], Step [285/704], Loss: 4.1023\n",
      "Epoch [1/20], Step [286/704], Loss: 3.8487\n",
      "Epoch [1/20], Step [287/704], Loss: 3.8861\n",
      "Epoch [1/20], Step [288/704], Loss: 3.9620\n",
      "Epoch [1/20], Step [289/704], Loss: 3.9974\n",
      "Epoch [1/20], Step [290/704], Loss: 3.9944\n",
      "Epoch [1/20], Step [291/704], Loss: 3.9053\n",
      "Epoch [1/20], Step [292/704], Loss: 4.0489\n",
      "Epoch [1/20], Step [293/704], Loss: 3.9905\n",
      "Epoch [1/20], Step [294/704], Loss: 4.0344\n",
      "Epoch [1/20], Step [295/704], Loss: 4.0408\n",
      "Epoch [1/20], Step [296/704], Loss: 3.9248\n",
      "Epoch [1/20], Step [297/704], Loss: 3.9492\n",
      "Epoch [1/20], Step [298/704], Loss: 3.9141\n",
      "Epoch [1/20], Step [299/704], Loss: 4.0682\n",
      "Epoch [1/20], Step [300/704], Loss: 3.9772\n",
      "Epoch [1/20], Step [301/704], Loss: 3.9408\n",
      "Epoch [1/20], Step [302/704], Loss: 4.2332\n",
      "Epoch [1/20], Step [303/704], Loss: 4.1477\n",
      "Epoch [1/20], Step [304/704], Loss: 4.0554\n",
      "Epoch [1/20], Step [305/704], Loss: 3.9435\n",
      "Epoch [1/20], Step [306/704], Loss: 3.9130\n",
      "Epoch [1/20], Step [307/704], Loss: 3.7715\n",
      "Epoch [1/20], Step [308/704], Loss: 4.1066\n",
      "Epoch [1/20], Step [309/704], Loss: 4.0513\n",
      "Epoch [1/20], Step [310/704], Loss: 3.9232\n",
      "Epoch [1/20], Step [311/704], Loss: 3.9800\n",
      "Epoch [1/20], Step [312/704], Loss: 4.1914\n",
      "Epoch [1/20], Step [313/704], Loss: 4.0143\n",
      "Epoch [1/20], Step [314/704], Loss: 4.0690\n",
      "Epoch [1/20], Step [315/704], Loss: 3.8440\n",
      "Epoch [1/20], Step [316/704], Loss: 3.9194\n",
      "Epoch [1/20], Step [317/704], Loss: 3.8545\n",
      "Epoch [1/20], Step [318/704], Loss: 3.9039\n",
      "Epoch [1/20], Step [319/704], Loss: 3.8575\n",
      "Epoch [1/20], Step [320/704], Loss: 3.9022\n",
      "Epoch [1/20], Step [321/704], Loss: 3.9605\n",
      "Epoch [1/20], Step [322/704], Loss: 3.9427\n",
      "Epoch [1/20], Step [323/704], Loss: 3.8881\n",
      "Epoch [1/20], Step [324/704], Loss: 3.9056\n",
      "Epoch [1/20], Step [325/704], Loss: 3.8832\n",
      "Epoch [1/20], Step [326/704], Loss: 3.7893\n",
      "Epoch [1/20], Step [327/704], Loss: 4.1102\n",
      "Epoch [1/20], Step [328/704], Loss: 4.0686\n",
      "Epoch [1/20], Step [329/704], Loss: 3.8324\n",
      "Epoch [1/20], Step [330/704], Loss: 4.0744\n",
      "Epoch [1/20], Step [331/704], Loss: 4.0684\n",
      "Epoch [1/20], Step [332/704], Loss: 3.8278\n",
      "Epoch [1/20], Step [333/704], Loss: 4.1052\n",
      "Epoch [1/20], Step [334/704], Loss: 3.8005\n",
      "Epoch [1/20], Step [335/704], Loss: 3.8052\n",
      "Epoch [1/20], Step [336/704], Loss: 3.7696\n",
      "Epoch [1/20], Step [337/704], Loss: 3.7684\n",
      "Epoch [1/20], Step [338/704], Loss: 3.8898\n",
      "Epoch [1/20], Step [339/704], Loss: 3.9502\n",
      "Epoch [1/20], Step [340/704], Loss: 3.8809\n",
      "Epoch [1/20], Step [341/704], Loss: 4.0253\n",
      "Epoch [1/20], Step [342/704], Loss: 4.0005\n",
      "Epoch [1/20], Step [343/704], Loss: 4.2071\n",
      "Epoch [1/20], Step [344/704], Loss: 4.0092\n",
      "Epoch [1/20], Step [345/704], Loss: 4.0211\n",
      "Epoch [1/20], Step [346/704], Loss: 3.8486\n",
      "Epoch [1/20], Step [347/704], Loss: 3.6192\n",
      "Epoch [1/20], Step [348/704], Loss: 3.7369\n",
      "Epoch [1/20], Step [349/704], Loss: 3.8723\n",
      "Epoch [1/20], Step [350/704], Loss: 3.8475\n",
      "Epoch [1/20], Step [351/704], Loss: 4.0726\n",
      "Epoch [1/20], Step [352/704], Loss: 4.1200\n",
      "Epoch [1/20], Step [353/704], Loss: 4.2505\n",
      "Epoch [1/20], Step [354/704], Loss: 3.6743\n",
      "Epoch [1/20], Step [355/704], Loss: 3.9802\n",
      "Epoch [1/20], Step [356/704], Loss: 4.0476\n",
      "Epoch [1/20], Step [357/704], Loss: 3.8808\n",
      "Epoch [1/20], Step [358/704], Loss: 4.0160\n",
      "Epoch [1/20], Step [359/704], Loss: 3.7789\n",
      "Epoch [1/20], Step [360/704], Loss: 3.9401\n",
      "Epoch [1/20], Step [361/704], Loss: 4.0311\n",
      "Epoch [1/20], Step [362/704], Loss: 3.7957\n",
      "Epoch [1/20], Step [363/704], Loss: 4.0469\n",
      "Epoch [1/20], Step [364/704], Loss: 3.9202\n",
      "Epoch [1/20], Step [365/704], Loss: 3.9095\n",
      "Epoch [1/20], Step [366/704], Loss: 3.7579\n",
      "Epoch [1/20], Step [367/704], Loss: 3.8357\n",
      "Epoch [1/20], Step [368/704], Loss: 3.8964\n",
      "Epoch [1/20], Step [369/704], Loss: 3.8772\n",
      "Epoch [1/20], Step [370/704], Loss: 3.8752\n",
      "Epoch [1/20], Step [371/704], Loss: 3.7986\n",
      "Epoch [1/20], Step [372/704], Loss: 3.8303\n",
      "Epoch [1/20], Step [373/704], Loss: 3.9111\n",
      "Epoch [1/20], Step [374/704], Loss: 3.9090\n",
      "Epoch [1/20], Step [375/704], Loss: 3.8573\n",
      "Epoch [1/20], Step [376/704], Loss: 4.0319\n",
      "Epoch [1/20], Step [377/704], Loss: 3.8236\n",
      "Epoch [1/20], Step [378/704], Loss: 3.5489\n",
      "Epoch [1/20], Step [379/704], Loss: 4.0972\n",
      "Epoch [1/20], Step [380/704], Loss: 3.7961\n",
      "Epoch [1/20], Step [381/704], Loss: 3.8381\n",
      "Epoch [1/20], Step [382/704], Loss: 3.7998\n",
      "Epoch [1/20], Step [383/704], Loss: 3.9603\n",
      "Epoch [1/20], Step [384/704], Loss: 3.9107\n",
      "Epoch [1/20], Step [385/704], Loss: 3.8275\n",
      "Epoch [1/20], Step [386/704], Loss: 4.1419\n",
      "Epoch [1/20], Step [387/704], Loss: 4.2497\n",
      "Epoch [1/20], Step [388/704], Loss: 4.1390\n",
      "Epoch [1/20], Step [389/704], Loss: 3.7392\n",
      "Epoch [1/20], Step [390/704], Loss: 3.7590\n",
      "Epoch [1/20], Step [391/704], Loss: 3.8829\n",
      "Epoch [1/20], Step [392/704], Loss: 3.7985\n",
      "Epoch [1/20], Step [393/704], Loss: 3.7445\n",
      "Epoch [1/20], Step [394/704], Loss: 3.9747\n",
      "Epoch [1/20], Step [395/704], Loss: 3.7587\n",
      "Epoch [1/20], Step [396/704], Loss: 3.9938\n",
      "Epoch [1/20], Step [397/704], Loss: 3.8148\n",
      "Epoch [1/20], Step [398/704], Loss: 3.6512\n",
      "Epoch [1/20], Step [399/704], Loss: 3.9068\n",
      "Epoch [1/20], Step [400/704], Loss: 3.6861\n",
      "Epoch [1/20], Step [401/704], Loss: 3.8671\n",
      "Epoch [1/20], Step [402/704], Loss: 3.7354\n",
      "Epoch [1/20], Step [403/704], Loss: 3.8948\n",
      "Epoch [1/20], Step [404/704], Loss: 4.0561\n",
      "Epoch [1/20], Step [405/704], Loss: 3.8487\n",
      "Epoch [1/20], Step [406/704], Loss: 3.5727\n",
      "Epoch [1/20], Step [407/704], Loss: 3.7527\n",
      "Epoch [1/20], Step [408/704], Loss: 3.9442\n",
      "Epoch [1/20], Step [409/704], Loss: 3.7357\n",
      "Epoch [1/20], Step [410/704], Loss: 3.7492\n",
      "Epoch [1/20], Step [411/704], Loss: 3.5294\n",
      "Epoch [1/20], Step [412/704], Loss: 3.8062\n",
      "Epoch [1/20], Step [413/704], Loss: 3.9291\n",
      "Epoch [1/20], Step [414/704], Loss: 3.8273\n",
      "Epoch [1/20], Step [415/704], Loss: 3.7183\n",
      "Epoch [1/20], Step [416/704], Loss: 3.5613\n",
      "Epoch [1/20], Step [417/704], Loss: 3.8031\n",
      "Epoch [1/20], Step [418/704], Loss: 3.5522\n",
      "Epoch [1/20], Step [419/704], Loss: 3.8135\n",
      "Epoch [1/20], Step [420/704], Loss: 3.7345\n",
      "Epoch [1/20], Step [421/704], Loss: 3.8070\n",
      "Epoch [1/20], Step [422/704], Loss: 3.7302\n",
      "Epoch [1/20], Step [423/704], Loss: 3.7662\n",
      "Epoch [1/20], Step [424/704], Loss: 3.6249\n",
      "Epoch [1/20], Step [425/704], Loss: 3.7157\n",
      "Epoch [1/20], Step [426/704], Loss: 4.0024\n",
      "Epoch [1/20], Step [427/704], Loss: 3.7991\n",
      "Epoch [1/20], Step [428/704], Loss: 3.9950\n",
      "Epoch [1/20], Step [429/704], Loss: 4.0018\n",
      "Epoch [1/20], Step [430/704], Loss: 3.7918\n",
      "Epoch [1/20], Step [431/704], Loss: 3.9599\n",
      "Epoch [1/20], Step [432/704], Loss: 3.9378\n",
      "Epoch [1/20], Step [433/704], Loss: 3.7562\n",
      "Epoch [1/20], Step [434/704], Loss: 4.0207\n",
      "Epoch [1/20], Step [435/704], Loss: 3.7207\n",
      "Epoch [1/20], Step [436/704], Loss: 3.8953\n",
      "Epoch [1/20], Step [437/704], Loss: 3.7820\n",
      "Epoch [1/20], Step [438/704], Loss: 3.5737\n",
      "Epoch [1/20], Step [439/704], Loss: 3.8222\n",
      "Epoch [1/20], Step [440/704], Loss: 3.6747\n",
      "Epoch [1/20], Step [441/704], Loss: 3.9640\n",
      "Epoch [1/20], Step [442/704], Loss: 3.7689\n",
      "Epoch [1/20], Step [443/704], Loss: 3.8176\n",
      "Epoch [1/20], Step [444/704], Loss: 3.5534\n",
      "Epoch [1/20], Step [445/704], Loss: 3.6796\n",
      "Epoch [1/20], Step [446/704], Loss: 3.8832\n",
      "Epoch [1/20], Step [447/704], Loss: 3.8323\n",
      "Epoch [1/20], Step [448/704], Loss: 3.4129\n",
      "Epoch [1/20], Step [449/704], Loss: 3.7033\n",
      "Epoch [1/20], Step [450/704], Loss: 3.5887\n",
      "Epoch [1/20], Step [451/704], Loss: 3.6523\n",
      "Epoch [1/20], Step [452/704], Loss: 3.7678\n",
      "Epoch [1/20], Step [453/704], Loss: 3.4550\n",
      "Epoch [1/20], Step [454/704], Loss: 3.8243\n",
      "Epoch [1/20], Step [455/704], Loss: 3.9166\n",
      "Epoch [1/20], Step [456/704], Loss: 3.7466\n",
      "Epoch [1/20], Step [457/704], Loss: 3.7767\n",
      "Epoch [1/20], Step [458/704], Loss: 3.6535\n",
      "Epoch [1/20], Step [459/704], Loss: 3.4931\n",
      "Epoch [1/20], Step [460/704], Loss: 3.6880\n",
      "Epoch [1/20], Step [461/704], Loss: 3.9517\n",
      "Epoch [1/20], Step [462/704], Loss: 3.6639\n",
      "Epoch [1/20], Step [463/704], Loss: 3.5759\n",
      "Epoch [1/20], Step [464/704], Loss: 4.1030\n",
      "Epoch [1/20], Step [465/704], Loss: 3.9432\n",
      "Epoch [1/20], Step [466/704], Loss: 3.9907\n",
      "Epoch [1/20], Step [467/704], Loss: 3.8615\n",
      "Epoch [1/20], Step [468/704], Loss: 3.7212\n",
      "Epoch [1/20], Step [469/704], Loss: 3.9478\n",
      "Epoch [1/20], Step [470/704], Loss: 3.5318\n",
      "Epoch [1/20], Step [471/704], Loss: 3.7229\n",
      "Epoch [1/20], Step [472/704], Loss: 3.7337\n",
      "Epoch [1/20], Step [473/704], Loss: 3.8425\n",
      "Epoch [1/20], Step [474/704], Loss: 4.1567\n",
      "Epoch [1/20], Step [475/704], Loss: 3.7018\n",
      "Epoch [1/20], Step [476/704], Loss: 3.9962\n",
      "Epoch [1/20], Step [477/704], Loss: 3.5772\n",
      "Epoch [1/20], Step [478/704], Loss: 4.0482\n",
      "Epoch [1/20], Step [479/704], Loss: 3.8529\n",
      "Epoch [1/20], Step [480/704], Loss: 3.8633\n",
      "Epoch [1/20], Step [481/704], Loss: 3.9284\n",
      "Epoch [1/20], Step [482/704], Loss: 3.6955\n",
      "Epoch [1/20], Step [483/704], Loss: 3.6316\n",
      "Epoch [1/20], Step [484/704], Loss: 3.8325\n",
      "Epoch [1/20], Step [485/704], Loss: 3.7155\n",
      "Epoch [1/20], Step [486/704], Loss: 3.8677\n",
      "Epoch [1/20], Step [487/704], Loss: 3.7843\n",
      "Epoch [1/20], Step [488/704], Loss: 3.6317\n",
      "Epoch [1/20], Step [489/704], Loss: 3.7845\n",
      "Epoch [1/20], Step [490/704], Loss: 3.5180\n",
      "Epoch [1/20], Step [491/704], Loss: 3.5678\n",
      "Epoch [1/20], Step [492/704], Loss: 3.8346\n",
      "Epoch [1/20], Step [493/704], Loss: 3.7697\n",
      "Epoch [1/20], Step [494/704], Loss: 3.4990\n",
      "Epoch [1/20], Step [495/704], Loss: 3.6238\n",
      "Epoch [1/20], Step [496/704], Loss: 3.8143\n",
      "Epoch [1/20], Step [497/704], Loss: 3.9435\n",
      "Epoch [1/20], Step [498/704], Loss: 3.8011\n",
      "Epoch [1/20], Step [499/704], Loss: 3.3437\n",
      "Epoch [1/20], Step [500/704], Loss: 3.6963\n",
      "Epoch [1/20], Step [501/704], Loss: 3.5713\n",
      "Epoch [1/20], Step [502/704], Loss: 3.8690\n",
      "Epoch [1/20], Step [503/704], Loss: 3.8222\n",
      "Epoch [1/20], Step [504/704], Loss: 4.1617\n",
      "Epoch [1/20], Step [505/704], Loss: 3.5697\n",
      "Epoch [1/20], Step [506/704], Loss: 3.7307\n",
      "Epoch [1/20], Step [507/704], Loss: 3.7930\n",
      "Epoch [1/20], Step [508/704], Loss: 3.5409\n",
      "Epoch [1/20], Step [509/704], Loss: 3.7071\n",
      "Epoch [1/20], Step [510/704], Loss: 3.6517\n",
      "Epoch [1/20], Step [511/704], Loss: 3.9115\n",
      "Epoch [1/20], Step [512/704], Loss: 3.7069\n",
      "Epoch [1/20], Step [513/704], Loss: 3.7377\n",
      "Epoch [1/20], Step [514/704], Loss: 3.8096\n",
      "Epoch [1/20], Step [515/704], Loss: 3.9421\n",
      "Epoch [1/20], Step [516/704], Loss: 3.6496\n",
      "Epoch [1/20], Step [517/704], Loss: 3.8537\n",
      "Epoch [1/20], Step [518/704], Loss: 3.7506\n",
      "Epoch [1/20], Step [519/704], Loss: 3.8825\n",
      "Epoch [1/20], Step [520/704], Loss: 3.5373\n",
      "Epoch [1/20], Step [521/704], Loss: 3.8035\n",
      "Epoch [1/20], Step [522/704], Loss: 3.6321\n",
      "Epoch [1/20], Step [523/704], Loss: 3.7171\n",
      "Epoch [1/20], Step [524/704], Loss: 3.7321\n",
      "Epoch [1/20], Step [525/704], Loss: 3.8960\n",
      "Epoch [1/20], Step [526/704], Loss: 3.7484\n",
      "Epoch [1/20], Step [527/704], Loss: 3.6299\n",
      "Epoch [1/20], Step [528/704], Loss: 3.4822\n",
      "Epoch [1/20], Step [529/704], Loss: 3.6322\n",
      "Epoch [1/20], Step [530/704], Loss: 3.6131\n",
      "Epoch [1/20], Step [531/704], Loss: 3.5157\n",
      "Epoch [1/20], Step [532/704], Loss: 3.4825\n",
      "Epoch [1/20], Step [533/704], Loss: 3.8176\n",
      "Epoch [1/20], Step [534/704], Loss: 3.7749\n",
      "Epoch [1/20], Step [535/704], Loss: 3.9032\n",
      "Epoch [1/20], Step [536/704], Loss: 3.3298\n",
      "Epoch [1/20], Step [537/704], Loss: 4.0672\n",
      "Epoch [1/20], Step [538/704], Loss: 3.8000\n",
      "Epoch [1/20], Step [539/704], Loss: 3.7264\n",
      "Epoch [1/20], Step [540/704], Loss: 3.8769\n",
      "Epoch [1/20], Step [541/704], Loss: 3.8225\n",
      "Epoch [1/20], Step [542/704], Loss: 3.6574\n",
      "Epoch [1/20], Step [543/704], Loss: 3.8859\n",
      "Epoch [1/20], Step [544/704], Loss: 3.7447\n",
      "Epoch [1/20], Step [545/704], Loss: 3.7085\n",
      "Epoch [1/20], Step [546/704], Loss: 3.3851\n",
      "Epoch [1/20], Step [547/704], Loss: 3.6717\n",
      "Epoch [1/20], Step [548/704], Loss: 3.6774\n",
      "Epoch [1/20], Step [549/704], Loss: 3.8056\n",
      "Epoch [1/20], Step [550/704], Loss: 3.9758\n",
      "Epoch [1/20], Step [551/704], Loss: 3.4538\n",
      "Epoch [1/20], Step [552/704], Loss: 3.6999\n",
      "Epoch [1/20], Step [553/704], Loss: 3.5262\n",
      "Epoch [1/20], Step [554/704], Loss: 3.5384\n",
      "Epoch [1/20], Step [555/704], Loss: 3.6393\n",
      "Epoch [1/20], Step [556/704], Loss: 3.6515\n",
      "Epoch [1/20], Step [557/704], Loss: 3.5701\n",
      "Epoch [1/20], Step [558/704], Loss: 3.6931\n",
      "Epoch [1/20], Step [559/704], Loss: 3.7699\n",
      "Epoch [1/20], Step [560/704], Loss: 3.6665\n",
      "Epoch [1/20], Step [561/704], Loss: 3.7769\n",
      "Epoch [1/20], Step [562/704], Loss: 3.4305\n",
      "Epoch [1/20], Step [563/704], Loss: 3.6721\n",
      "Epoch [1/20], Step [564/704], Loss: 3.6569\n",
      "Epoch [1/20], Step [565/704], Loss: 3.7452\n",
      "Epoch [1/20], Step [566/704], Loss: 3.5775\n",
      "Epoch [1/20], Step [567/704], Loss: 3.9779\n",
      "Epoch [1/20], Step [568/704], Loss: 3.5410\n",
      "Epoch [1/20], Step [569/704], Loss: 3.6837\n",
      "Epoch [1/20], Step [570/704], Loss: 3.8504\n",
      "Epoch [1/20], Step [571/704], Loss: 3.7572\n",
      "Epoch [1/20], Step [572/704], Loss: 3.7702\n",
      "Epoch [1/20], Step [573/704], Loss: 3.4638\n",
      "Epoch [1/20], Step [574/704], Loss: 3.5298\n",
      "Epoch [1/20], Step [575/704], Loss: 3.8165\n",
      "Epoch [1/20], Step [576/704], Loss: 3.5951\n",
      "Epoch [1/20], Step [577/704], Loss: 3.6745\n",
      "Epoch [1/20], Step [578/704], Loss: 3.3486\n",
      "Epoch [1/20], Step [579/704], Loss: 3.5917\n",
      "Epoch [1/20], Step [580/704], Loss: 3.4975\n",
      "Epoch [1/20], Step [581/704], Loss: 3.4684\n",
      "Epoch [1/20], Step [582/704], Loss: 3.6958\n",
      "Epoch [1/20], Step [583/704], Loss: 3.4691\n",
      "Epoch [1/20], Step [584/704], Loss: 3.4489\n",
      "Epoch [1/20], Step [585/704], Loss: 3.5782\n",
      "Epoch [1/20], Step [586/704], Loss: 3.4800\n",
      "Epoch [1/20], Step [587/704], Loss: 3.4071\n",
      "Epoch [1/20], Step [588/704], Loss: 3.2707\n",
      "Epoch [1/20], Step [589/704], Loss: 3.4632\n",
      "Epoch [1/20], Step [590/704], Loss: 3.4507\n",
      "Epoch [1/20], Step [591/704], Loss: 3.7662\n",
      "Epoch [1/20], Step [592/704], Loss: 3.5763\n",
      "Epoch [1/20], Step [593/704], Loss: 3.5822\n",
      "Epoch [1/20], Step [594/704], Loss: 3.4664\n",
      "Epoch [1/20], Step [595/704], Loss: 3.2648\n",
      "Epoch [1/20], Step [596/704], Loss: 3.6080\n",
      "Epoch [1/20], Step [597/704], Loss: 3.7564\n",
      "Epoch [1/20], Step [598/704], Loss: 3.2820\n",
      "Epoch [1/20], Step [599/704], Loss: 3.4737\n",
      "Epoch [1/20], Step [600/704], Loss: 3.5210\n",
      "Epoch [1/20], Step [601/704], Loss: 3.6745\n",
      "Epoch [1/20], Step [602/704], Loss: 3.5672\n",
      "Epoch [1/20], Step [603/704], Loss: 3.4310\n",
      "Epoch [1/20], Step [604/704], Loss: 3.9032\n",
      "Epoch [1/20], Step [605/704], Loss: 3.4072\n",
      "Epoch [1/20], Step [606/704], Loss: 3.7929\n",
      "Epoch [1/20], Step [607/704], Loss: 3.9997\n",
      "Epoch [1/20], Step [608/704], Loss: 3.6010\n",
      "Epoch [1/20], Step [609/704], Loss: 3.3464\n",
      "Epoch [1/20], Step [610/704], Loss: 3.5315\n",
      "Epoch [1/20], Step [611/704], Loss: 3.5666\n",
      "Epoch [1/20], Step [612/704], Loss: 3.6615\n",
      "Epoch [1/20], Step [613/704], Loss: 3.6575\n",
      "Epoch [1/20], Step [614/704], Loss: 3.7585\n",
      "Epoch [1/20], Step [615/704], Loss: 3.7004\n",
      "Epoch [1/20], Step [616/704], Loss: 3.5590\n",
      "Epoch [1/20], Step [617/704], Loss: 3.8295\n",
      "Epoch [1/20], Step [618/704], Loss: 3.5138\n",
      "Epoch [1/20], Step [619/704], Loss: 3.5645\n",
      "Epoch [1/20], Step [620/704], Loss: 3.5340\n",
      "Epoch [1/20], Step [621/704], Loss: 3.7601\n",
      "Epoch [1/20], Step [622/704], Loss: 3.5752\n",
      "Epoch [1/20], Step [623/704], Loss: 3.4780\n",
      "Epoch [1/20], Step [624/704], Loss: 3.2732\n",
      "Epoch [1/20], Step [625/704], Loss: 3.4647\n",
      "Epoch [1/20], Step [626/704], Loss: 3.4361\n",
      "Epoch [1/20], Step [627/704], Loss: 3.6966\n",
      "Epoch [1/20], Step [628/704], Loss: 3.7774\n",
      "Epoch [1/20], Step [629/704], Loss: 3.2611\n",
      "Epoch [1/20], Step [630/704], Loss: 3.2534\n",
      "Epoch [1/20], Step [631/704], Loss: 3.6207\n",
      "Epoch [1/20], Step [632/704], Loss: 3.5148\n",
      "Epoch [1/20], Step [633/704], Loss: 3.5244\n",
      "Epoch [1/20], Step [634/704], Loss: 3.7785\n",
      "Epoch [1/20], Step [635/704], Loss: 3.5101\n",
      "Epoch [1/20], Step [636/704], Loss: 3.8063\n",
      "Epoch [1/20], Step [637/704], Loss: 3.7015\n",
      "Epoch [1/20], Step [638/704], Loss: 3.6111\n",
      "Epoch [1/20], Step [639/704], Loss: 3.3864\n",
      "Epoch [1/20], Step [640/704], Loss: 3.7345\n",
      "Epoch [1/20], Step [641/704], Loss: 3.8257\n",
      "Epoch [1/20], Step [642/704], Loss: 3.6024\n",
      "Epoch [1/20], Step [643/704], Loss: 3.6010\n",
      "Epoch [1/20], Step [644/704], Loss: 3.8394\n",
      "Epoch [1/20], Step [645/704], Loss: 3.4445\n",
      "Epoch [1/20], Step [646/704], Loss: 3.6733\n",
      "Epoch [1/20], Step [647/704], Loss: 3.6470\n",
      "Epoch [1/20], Step [648/704], Loss: 3.6029\n",
      "Epoch [1/20], Step [649/704], Loss: 3.5786\n",
      "Epoch [1/20], Step [650/704], Loss: 3.5392\n",
      "Epoch [1/20], Step [651/704], Loss: 3.8332\n",
      "Epoch [1/20], Step [652/704], Loss: 3.3525\n",
      "Epoch [1/20], Step [653/704], Loss: 3.4788\n",
      "Epoch [1/20], Step [654/704], Loss: 3.6342\n",
      "Epoch [1/20], Step [655/704], Loss: 3.4271\n",
      "Epoch [1/20], Step [656/704], Loss: 3.4895\n",
      "Epoch [1/20], Step [657/704], Loss: 3.6209\n",
      "Epoch [1/20], Step [658/704], Loss: 3.9248\n",
      "Epoch [1/20], Step [659/704], Loss: 3.4894\n",
      "Epoch [1/20], Step [660/704], Loss: 3.2977\n",
      "Epoch [1/20], Step [661/704], Loss: 3.5087\n",
      "Epoch [1/20], Step [662/704], Loss: 3.6203\n",
      "Epoch [1/20], Step [663/704], Loss: 3.1481\n",
      "Epoch [1/20], Step [664/704], Loss: 3.5422\n",
      "Epoch [1/20], Step [665/704], Loss: 3.6924\n",
      "Epoch [1/20], Step [666/704], Loss: 3.6799\n",
      "Epoch [1/20], Step [667/704], Loss: 3.8726\n",
      "Epoch [1/20], Step [668/704], Loss: 3.5316\n",
      "Epoch [1/20], Step [669/704], Loss: 3.5299\n",
      "Epoch [1/20], Step [670/704], Loss: 3.5231\n",
      "Epoch [1/20], Step [671/704], Loss: 3.4494\n",
      "Epoch [1/20], Step [672/704], Loss: 3.6195\n",
      "Epoch [1/20], Step [673/704], Loss: 3.5018\n",
      "Epoch [1/20], Step [674/704], Loss: 3.6060\n",
      "Epoch [1/20], Step [675/704], Loss: 3.2628\n",
      "Epoch [1/20], Step [676/704], Loss: 3.5435\n",
      "Epoch [1/20], Step [677/704], Loss: 3.7050\n",
      "Epoch [1/20], Step [678/704], Loss: 3.6837\n",
      "Epoch [1/20], Step [679/704], Loss: 3.5100\n",
      "Epoch [1/20], Step [680/704], Loss: 3.4816\n",
      "Epoch [1/20], Step [681/704], Loss: 3.3876\n",
      "Epoch [1/20], Step [682/704], Loss: 3.4601\n",
      "Epoch [1/20], Step [683/704], Loss: 3.4895\n",
      "Epoch [1/20], Step [684/704], Loss: 3.5164\n",
      "Epoch [1/20], Step [685/704], Loss: 3.5236\n",
      "Epoch [1/20], Step [686/704], Loss: 3.5223\n",
      "Epoch [1/20], Step [687/704], Loss: 3.4783\n",
      "Epoch [1/20], Step [688/704], Loss: 3.8543\n",
      "Epoch [1/20], Step [689/704], Loss: 3.4441\n",
      "Epoch [1/20], Step [690/704], Loss: 3.6542\n",
      "Epoch [1/20], Step [691/704], Loss: 3.3030\n",
      "Epoch [1/20], Step [692/704], Loss: 3.7553\n",
      "Epoch [1/20], Step [693/704], Loss: 3.7104\n",
      "Epoch [1/20], Step [694/704], Loss: 3.4576\n",
      "Epoch [1/20], Step [695/704], Loss: 3.4843\n",
      "Epoch [1/20], Step [696/704], Loss: 3.7161\n",
      "Epoch [1/20], Step [697/704], Loss: 3.8659\n",
      "Epoch [1/20], Step [698/704], Loss: 3.5420\n",
      "Epoch [1/20], Step [699/704], Loss: 3.4252\n",
      "Epoch [1/20], Step [700/704], Loss: 3.4252\n",
      "Epoch [1/20], Step [701/704], Loss: 3.8073\n",
      "Epoch [1/20], Step [702/704], Loss: 3.5470\n",
      "Epoch [1/20], Step [703/704], Loss: 3.3843\n",
      "Epoch [1/20], Step [704/704], Loss: 4.1979\n",
      "Accuracy of the network on the [5000] validation images: [1.14 %]\n",
      "Epoch [2/20], Step [1/704], Loss: 3.2683\n",
      "Epoch [2/20], Step [2/704], Loss: 3.6010\n",
      "Epoch [2/20], Step [3/704], Loss: 3.7209\n",
      "Epoch [2/20], Step [4/704], Loss: 3.4442\n",
      "Epoch [2/20], Step [5/704], Loss: 3.9129\n",
      "Epoch [2/20], Step [6/704], Loss: 3.5727\n",
      "Epoch [2/20], Step [7/704], Loss: 3.6732\n",
      "Epoch [2/20], Step [8/704], Loss: 3.5934\n",
      "Epoch [2/20], Step [9/704], Loss: 3.8208\n",
      "Epoch [2/20], Step [10/704], Loss: 3.4302\n",
      "Epoch [2/20], Step [11/704], Loss: 3.3899\n",
      "Epoch [2/20], Step [12/704], Loss: 3.4420\n",
      "Epoch [2/20], Step [13/704], Loss: 3.1811\n",
      "Epoch [2/20], Step [14/704], Loss: 3.7555\n",
      "Epoch [2/20], Step [15/704], Loss: 3.2909\n",
      "Epoch [2/20], Step [16/704], Loss: 3.4527\n",
      "Epoch [2/20], Step [17/704], Loss: 3.6484\n",
      "Epoch [2/20], Step [18/704], Loss: 3.4986\n",
      "Epoch [2/20], Step [19/704], Loss: 3.5834\n",
      "Epoch [2/20], Step [20/704], Loss: 3.3719\n",
      "Epoch [2/20], Step [21/704], Loss: 3.4572\n",
      "Epoch [2/20], Step [22/704], Loss: 3.6332\n",
      "Epoch [2/20], Step [23/704], Loss: 3.5263\n",
      "Epoch [2/20], Step [24/704], Loss: 3.9321\n",
      "Epoch [2/20], Step [25/704], Loss: 3.4153\n",
      "Epoch [2/20], Step [26/704], Loss: 3.8731\n",
      "Epoch [2/20], Step [27/704], Loss: 3.6120\n",
      "Epoch [2/20], Step [28/704], Loss: 3.2469\n",
      "Epoch [2/20], Step [29/704], Loss: 3.5769\n",
      "Epoch [2/20], Step [30/704], Loss: 3.5409\n",
      "Epoch [2/20], Step [31/704], Loss: 3.4584\n",
      "Epoch [2/20], Step [32/704], Loss: 3.4943\n",
      "Epoch [2/20], Step [33/704], Loss: 3.3514\n",
      "Epoch [2/20], Step [34/704], Loss: 3.4571\n",
      "Epoch [2/20], Step [35/704], Loss: 3.4781\n",
      "Epoch [2/20], Step [36/704], Loss: 3.3400\n",
      "Epoch [2/20], Step [37/704], Loss: 3.2766\n",
      "Epoch [2/20], Step [38/704], Loss: 3.2938\n",
      "Epoch [2/20], Step [39/704], Loss: 3.1552\n",
      "Epoch [2/20], Step [40/704], Loss: 3.3639\n",
      "Epoch [2/20], Step [41/704], Loss: 3.6088\n",
      "Epoch [2/20], Step [42/704], Loss: 3.3205\n",
      "Epoch [2/20], Step [43/704], Loss: 3.0035\n",
      "Epoch [2/20], Step [44/704], Loss: 3.5144\n",
      "Epoch [2/20], Step [45/704], Loss: 3.4193\n",
      "Epoch [2/20], Step [46/704], Loss: 3.4833\n",
      "Epoch [2/20], Step [47/704], Loss: 3.2816\n",
      "Epoch [2/20], Step [48/704], Loss: 2.9913\n",
      "Epoch [2/20], Step [49/704], Loss: 3.4110\n",
      "Epoch [2/20], Step [50/704], Loss: 3.4490\n",
      "Epoch [2/20], Step [51/704], Loss: 3.8314\n",
      "Epoch [2/20], Step [52/704], Loss: 3.2386\n",
      "Epoch [2/20], Step [53/704], Loss: 3.2829\n",
      "Epoch [2/20], Step [54/704], Loss: 3.3576\n",
      "Epoch [2/20], Step [55/704], Loss: 3.8709\n",
      "Epoch [2/20], Step [56/704], Loss: 3.6293\n",
      "Epoch [2/20], Step [57/704], Loss: 3.4735\n",
      "Epoch [2/20], Step [58/704], Loss: 3.7656\n",
      "Epoch [2/20], Step [59/704], Loss: 3.2644\n",
      "Epoch [2/20], Step [60/704], Loss: 3.5913\n",
      "Epoch [2/20], Step [61/704], Loss: 3.4287\n",
      "Epoch [2/20], Step [62/704], Loss: 3.4539\n",
      "Epoch [2/20], Step [63/704], Loss: 3.4327\n",
      "Epoch [2/20], Step [64/704], Loss: 3.4952\n",
      "Epoch [2/20], Step [65/704], Loss: 3.2205\n",
      "Epoch [2/20], Step [66/704], Loss: 3.5162\n",
      "Epoch [2/20], Step [67/704], Loss: 3.2898\n",
      "Epoch [2/20], Step [68/704], Loss: 3.1895\n",
      "Epoch [2/20], Step [69/704], Loss: 3.1754\n",
      "Epoch [2/20], Step [70/704], Loss: 3.5160\n",
      "Epoch [2/20], Step [71/704], Loss: 3.5202\n",
      "Epoch [2/20], Step [72/704], Loss: 3.3930\n",
      "Epoch [2/20], Step [73/704], Loss: 3.3683\n",
      "Epoch [2/20], Step [74/704], Loss: 3.1204\n",
      "Epoch [2/20], Step [75/704], Loss: 3.5181\n",
      "Epoch [2/20], Step [76/704], Loss: 3.2515\n",
      "Epoch [2/20], Step [77/704], Loss: 3.5671\n",
      "Epoch [2/20], Step [78/704], Loss: 3.3946\n",
      "Epoch [2/20], Step [79/704], Loss: 3.2500\n",
      "Epoch [2/20], Step [80/704], Loss: 3.4346\n",
      "Epoch [2/20], Step [81/704], Loss: 3.3043\n",
      "Epoch [2/20], Step [82/704], Loss: 3.6814\n",
      "Epoch [2/20], Step [83/704], Loss: 3.1299\n",
      "Epoch [2/20], Step [84/704], Loss: 3.3028\n",
      "Epoch [2/20], Step [85/704], Loss: 3.4805\n",
      "Epoch [2/20], Step [86/704], Loss: 3.6497\n",
      "Epoch [2/20], Step [87/704], Loss: 3.3662\n",
      "Epoch [2/20], Step [88/704], Loss: 3.7947\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a8cd1768a748>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print (f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: [{loss.item():.4f}]\")\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "\n",
    "        print(f\"Accuracy of the network on the [{5000}] validation images: [{100 * correct / total} %]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae754e44-0717-4c28-9bd4-9277958059f3",
   "metadata": {
    "id": "ae754e44-0717-4c28-9bd4-9277958059f3"
   },
   "source": [
    "## آزمایش - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39509b8-8ebd-48ee-ad13-23e1aa72b337",
   "metadata": {
    "id": "c39509b8-8ebd-48ee-ad13-23e1aa72b337"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print(f\"Accuracy of the network on the [{10000}] test images: [{100 * correct / total   } %]\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
